{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d95a701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "from scipy.ndimage import zoom\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, LSTM, TimeDistributed\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a267cf",
   "metadata": {},
   "source": [
    "Check Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "939842f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drowsy path exists: True\n",
      "Emotion path exists: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set correct base paths\n",
    "DROWSY_PATH = os.path.join(\"D:\\VSCODE\\DDD_project\\DatasetNew\", \"DrowsySet\")\n",
    "EMOTION_PATH = os.path.join(\"D:\\VSCODE\\DDD_project\\DatasetNew\", \"Emotion\")\n",
    "\n",
    "# Confirm folder existence\n",
    "print(\"Drowsy path exists:\", os.path.exists(DROWSY_PATH))\n",
    "print(\"Emotion path exists:\", os.path.exists(EMOTION_PATH))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40ee6d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DROWSY_PATH = \"D:\\VSCODE\\DDD_project\\DatasetNew\\DrowsySet\"\n",
    "EMOTION_PATH = \"D:\\VSCODE\\DDD_project\\DatasetNew\\Emotion\"\n",
    "\n",
    "# Drowsy: ['Active', 'Fatigue']\n",
    "# Emotion: ['Anger', 'Fear', 'Happy', 'Sad', 'Surprise']\n",
    "\n",
    "DROWSY_CLASSES = [\"Active\", \"Fatigue\"]\n",
    "EMOTION_CLASSES = [\"Anger\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ea49b2",
   "metadata": {},
   "source": [
    "Shared preprocess function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d2b181a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def preprocess_list(x):\n",
    "    return int((x.split(\"-\")[2]).split(\".\")[0])  # Assumes consistent naming: subject-seq-frame.jpg\n",
    "\n",
    "def preprocess_dict(x):\n",
    "    res = list(np.argsort(list(map(preprocess_list, x))))\n",
    "    return [x[i] for i in res]\n",
    "\n",
    "def img2array(files, path, size=(48, 48)):\n",
    "    images = []\n",
    "    for file in files:\n",
    "        img = cv2.imread(os.path.join(path, file), cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, size)\n",
    "            images.append(img)\n",
    "    return np.array(images)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22438e7a",
   "metadata": {},
   "source": [
    "Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bfb4436",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "drowsy_data = defaultdict(lambda: defaultdict(list))\n",
    "emotion_data = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "# Load Drowsy Data\n",
    "for label in DROWSY_CLASSES:\n",
    "    folder_path = os.path.join(DROWSY_PATH, label)\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"âŒ Missing folder: {folder_path}\")\n",
    "    else:\n",
    "        for f in os.listdir(folder_path):\n",
    "            subject = f.split(\"-\")[0]\n",
    "            drowsy_data[label][subject].append(f)\n",
    "\n",
    "# Load Emotion Data\n",
    "for label in EMOTION_CLASSES:\n",
    "    folder_path = os.path.join(EMOTION_PATH, label)\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"âŒ Missing folder: {folder_path}\")\n",
    "    else:\n",
    "        for f in os.listdir(folder_path):\n",
    "            subject = f.split(\"-\")[0]\n",
    "            emotion_data[label][subject].append(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5ae2a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŸ¡ DrowsySet File Count:\n",
      "  Active: 4560 files\n",
      "  Fatigue: 4560 files\n",
      "\n",
      "ğŸŸ¢ Emotion File Count:\n",
      "  Anger: 135 files\n",
      "  Fear: 75 files\n",
      "  Happy: 207 files\n",
      "  Sad: 84 files\n",
      "  Surprise: 249 files\n"
     ]
    }
   ],
   "source": [
    "# --- Drowsy Set ---\n",
    "print(\"ğŸŸ¡ DrowsySet File Count:\")\n",
    "for label in DROWSY_CLASSES:\n",
    "    folder_path = os.path.join(DROWSY_PATH, label)\n",
    "    if os.path.exists(folder_path):\n",
    "        file_count = len(os.listdir(folder_path))\n",
    "        print(f\"  {label}: {file_count} files\")\n",
    "    else:\n",
    "        print(f\"  âŒ Folder not found: {folder_path}\")\n",
    "\n",
    "# --- Emotion Set ---\n",
    "print(\"\\nğŸŸ¢ Emotion File Count:\")\n",
    "for label in EMOTION_CLASSES:\n",
    "    folder_path = os.path.join(EMOTION_PATH, label)\n",
    "    if os.path.exists(folder_path):\n",
    "        file_count = len(os.listdir(folder_path))\n",
    "        print(f\"  {label}: {file_count} files\")\n",
    "    else:\n",
    "        print(f\"  âŒ Folder not found: {folder_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb4cf250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¢ Total files - DrowsySet: 9120, Emotion: 750\n"
     ]
    }
   ],
   "source": [
    "total_drowsy = sum(len(os.listdir(os.path.join(DROWSY_PATH, label))) for label in DROWSY_CLASSES if os.path.exists(os.path.join(DROWSY_PATH, label)))\n",
    "total_emotion = sum(len(os.listdir(os.path.join(EMOTION_PATH, label))) for label in EMOTION_CLASSES if os.path.exists(os.path.join(EMOTION_PATH, label)))\n",
    "print(f\"\\nğŸ”¢ Total files - DrowsySet: {total_drowsy}, Emotion: {total_emotion}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd6a69e",
   "metadata": {},
   "source": [
    "Convert Dataset to grayscale and uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bacdc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2  # OpenCV for image processing\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_image_sequence_from_folder(folder_path):\n",
    "    \"\"\"Load and return all images from a given folder as grayscale 48x48.\"\"\"\n",
    "    frames = []\n",
    "    for filename in sorted(os.listdir(folder_path)):\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        img = cv2.imread(img_path)  # Reads in color (BGR)\n",
    "        if img is None:\n",
    "            continue\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
    "        img = cv2.resize(img, (48, 48))              # Resize if needed\n",
    "        frames.append(img)\n",
    "    return np.array(frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f50f88ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Loading Active from D:\\VSCODE\\DDD_project\\DatasetNew\\DrowsySet\\Active\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4560/4560 [00:39<00:00, 116.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Loading Fatigue from D:\\VSCODE\\DDD_project\\DatasetNew\\DrowsySet\\Fatigue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4560/4560 [00:35<00:00, 129.71it/s]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "drowsy_data = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for label in DROWSY_CLASSES:  # [\"Active\", \"Fatigue\"]\n",
    "    folder_path = os.path.join(DROWSY_PATH, label)\n",
    "    if os.path.exists(folder_path):\n",
    "        print(f\"ğŸ“‚ Loading {label} from {folder_path}\")\n",
    "        \n",
    "        filenames = sorted(os.listdir(folder_path))  # âœ… Sort files\n",
    "        \n",
    "        for f in tqdm(filenames):\n",
    "            subject = f.split(\"-\")[0]  # Or however your subjects are separated\n",
    "            f_path = os.path.join(folder_path, f)\n",
    "            img = cv2.imread(f_path)\n",
    "            if img is None:\n",
    "                continue\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # Grayscale\n",
    "            img = cv2.resize(img, (48, 48))              # Resize to uniform size\n",
    "            drowsy_data[label][subject].append(img)\n",
    "    else:\n",
    "        print(f\"âŒ Missing folder: {folder_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71336d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Loading Anger from D:\\VSCODE\\DDD_project\\DatasetNew\\Emotion\\Anger\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135/135 [00:00<00:00, 3745.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Loading Fear from D:\\VSCODE\\DDD_project\\DatasetNew\\Emotion\\Fear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:00<00:00, 3947.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Loading Happy from D:\\VSCODE\\DDD_project\\DatasetNew\\Emotion\\Happy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 207/207 [00:00<00:00, 3392.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Loading Sad from D:\\VSCODE\\DDD_project\\DatasetNew\\Emotion\\Sad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:00<00:00, 2624.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Loading Surprise from D:\\VSCODE\\DDD_project\\DatasetNew\\Emotion\\Surprise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 249/249 [00:00<00:00, 4290.96it/s]\n"
     ]
    }
   ],
   "source": [
    "emotion_data = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for label in EMOTION_CLASSES:  # [\"Anger\", \"fear\", \"happy\", \"sad\", \"surprise\"]\n",
    "    folder_path = os.path.join(EMOTION_PATH, label)\n",
    "    if os.path.exists(folder_path):\n",
    "        print(f\"ğŸ“‚ Loading {label} from {folder_path}\")\n",
    "        for f in tqdm(os.listdir(folder_path)):\n",
    "            subject = f.split(\"-\")[0]\n",
    "            f_path = os.path.join(folder_path, f)\n",
    "            img = cv2.imread(f_path)  # Load image (even if already grayscale, OpenCV returns 3D sometimes)\n",
    "            if img is None:\n",
    "                continue\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # Convert to grayscale (idempotent if already grayscale)\n",
    "            img = cv2.resize(img, (48, 48))              # Resize\n",
    "            emotion_data[label][subject].append(img)\n",
    "    else:\n",
    "        print(f\"âŒ Missing folder: {folder_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000a6023",
   "metadata": {},
   "source": [
    "Pre LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "550e4069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define max sequence length\n",
    "SEQ_LENGTH = 10\n",
    "\n",
    "def pad_or_truncate(seq, desired_len=SEQ_LENGTH):\n",
    "    if len(seq) < desired_len:\n",
    "        # Pad with zeros\n",
    "        pad_len = desired_len - len(seq)\n",
    "        padding = np.zeros((pad_len, 48, 48), dtype=np.uint8)\n",
    "        return np.concatenate([seq, padding], axis=0)\n",
    "    else:\n",
    "        # Truncate\n",
    "        return seq[:desired_len]\n",
    "\n",
    "def process_dataset(data_dict, label_map):\n",
    "    X, y = [], []\n",
    "    for label, subjects in data_dict.items():\n",
    "        for subj_frames in subjects.values():\n",
    "            seq = pad_or_truncate(subj_frames)\n",
    "            X.append(seq)\n",
    "            y.append(label_map[label])\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f994edd6",
   "metadata": {},
   "source": [
    "DrowsySet Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96bf7ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DROWSY_LABELS = {\"Active\": 0, \"Fatigue\": 1}\n",
    "X_drowsy, y_drowsy = process_dataset(drowsy_data, DROWSY_LABELS)\n",
    "\n",
    "# Normalize images\n",
    "X_drowsy = X_drowsy / 255.0\n",
    "\n",
    "# Flatten to (samples, timesteps, features)\n",
    "X_drowsy = X_drowsy.reshape((X_drowsy.shape[0], SEQ_LENGTH, -1))\n",
    "\n",
    "# Train/Test Split\n",
    "X_drowsy_train, X_drowsy_test, y_drowsy_train, y_drowsy_test = train_test_split(\n",
    "    X_drowsy, y_drowsy, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5956ce0",
   "metadata": {},
   "source": [
    "EmotionSet Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6bd9835",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMOTION_LABELS = {\"Anger\": 0, \"Fear\": 1, \"Happy\": 2, \"Sad\": 3, \"Surprise\": 4}\n",
    "X_emotion, y_emotion = process_dataset(emotion_data, EMOTION_LABELS)\n",
    "\n",
    "# Normalize and reshape\n",
    "X_emotion = X_emotion / 255.0\n",
    "X_emotion = X_emotion.reshape((X_emotion.shape[0], SEQ_LENGTH, -1))\n",
    "\n",
    "# One-hot encode labels\n",
    "y_emotion_cat = to_categorical(y_emotion, num_classes=5)\n",
    "\n",
    "# Train/Test Split\n",
    "X_emotion_train, X_emotion_test, y_emotion_train, y_emotion_test = train_test_split(\n",
    "    X_emotion, y_emotion_cat, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ea202d",
   "metadata": {},
   "source": [
    "Drowsiness detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d6265d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\VSCODE\\DDD_project\\env\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">606,464</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)         â”‚       \u001b[38;5;34m606,464\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚        \u001b[38;5;34m12,416\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             â”‚           \u001b[38;5;34m528\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              â”‚            \u001b[38;5;34m17\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">619,425</span> (2.36 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m619,425\u001b[0m (2.36 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">619,425</span> (2.36 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m619,425\u001b[0m (2.36 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "model_drowsy = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(SEQ_LENGTH, 48*48)),\n",
    "    LSTM(32),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_drowsy.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_drowsy.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c835f2",
   "metadata": {},
   "source": [
    "Emotion Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4f81dbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">606,464</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,056</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">165</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)         â”‚       \u001b[38;5;34m606,464\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚        \u001b[38;5;34m12,416\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚         \u001b[38;5;34m1,056\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              â”‚           \u001b[38;5;34m165\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">620,101</span> (2.37 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m620,101\u001b[0m (2.37 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">620,101</span> (2.37 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m620,101\u001b[0m (2.37 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_emotion = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(SEQ_LENGTH, 48*48)),\n",
    "    LSTM(32),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "model_emotion.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_emotion.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "501bc474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m183/183\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.6527 - loss: 0.6113 - val_accuracy: 0.7603 - val_loss: 0.4275\n",
      "Epoch 2/10\n",
      "\u001b[1m183/183\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.7430 - loss: 0.4578 - val_accuracy: 0.7945 - val_loss: 0.3731\n",
      "Epoch 3/10\n",
      "\u001b[1m183/183\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.7784 - loss: 0.4054 - val_accuracy: 0.7822 - val_loss: 0.4128\n",
      "Epoch 4/10\n",
      "\u001b[1m183/183\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.7697 - loss: 0.4092 - val_accuracy: 0.8014 - val_loss: 0.3638\n",
      "Epoch 5/10\n",
      "\u001b[1m183/183\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.7980 - loss: 0.3862 - val_accuracy: 0.8007 - val_loss: 0.3687\n",
      "Epoch 6/10\n",
      "\u001b[1m183/183\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.8037 - loss: 0.3818 - val_accuracy: 0.7521 - val_loss: 0.4148\n",
      "Epoch 7/10\n",
      "\u001b[1m183/183\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.8250 - loss: 0.3724 - val_accuracy: 0.8014 - val_loss: 0.3654\n",
      "Epoch 8/10\n",
      "\u001b[1m183/183\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.8054 - loss: 0.3791 - val_accuracy: 0.8192 - val_loss: 0.3434\n",
      "Epoch 9/10\n",
      "\u001b[1m183/183\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.8194 - loss: 0.3519 - val_accuracy: 0.8205 - val_loss: 0.3434\n",
      "Epoch 10/10\n",
      "\u001b[1m183/183\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.8282 - loss: 0.3393 - val_accuracy: 0.8158 - val_loss: 0.3572\n",
      "Epoch 1/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - accuracy: 0.3300 - loss: 1.5474 - val_accuracy: 0.2667 - val_loss: 1.5481\n",
      "Epoch 2/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.3883 - loss: 1.4286 - val_accuracy: 0.5250 - val_loss: 1.3464\n",
      "Epoch 3/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6316 - loss: 1.1059 - val_accuracy: 0.5000 - val_loss: 1.1388\n",
      "Epoch 4/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6048 - loss: 1.0102 - val_accuracy: 0.5333 - val_loss: 1.1528\n",
      "Epoch 5/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6287 - loss: 0.9103 - val_accuracy: 0.5667 - val_loss: 1.0028\n",
      "Epoch 6/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6559 - loss: 0.8476 - val_accuracy: 0.5917 - val_loss: 0.9235\n",
      "Epoch 7/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6645 - loss: 0.7548 - val_accuracy: 0.6500 - val_loss: 0.8338\n",
      "Epoch 8/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7022 - loss: 0.7328 - val_accuracy: 0.6417 - val_loss: 0.8220\n",
      "Epoch 9/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7003 - loss: 0.7461 - val_accuracy: 0.6917 - val_loss: 0.8019\n",
      "Epoch 10/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7141 - loss: 0.7120 - val_accuracy: 0.7583 - val_loss: 0.6859\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1866c4c4a10>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drowsy Model\n",
    "model_drowsy.fit(X_drowsy_train, y_drowsy_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Emotion Model\n",
    "model_emotion.fit(X_emotion_train, y_emotion_train, epochs=10, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9a49e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_driver_state(X_seq):\n",
    "    X_seq = X_seq / 255.0\n",
    "    X_seq = X_seq.reshape((1, SEQ_LENGTH, 48*48))\n",
    "\n",
    "    drowsy_pred = model_drowsy.predict(X_seq)[0][0]\n",
    "\n",
    "    if drowsy_pred >= 0.5:\n",
    "        emotion_pred = model_emotion.predict(X_seq)\n",
    "        emotion_label = np.argmax(emotion_pred)\n",
    "        print(f\"ğŸŸ¢ Status: Active - Emotion: {list(EMOTION_LABELS.keys())[emotion_label]}\")\n",
    "    else:\n",
    "        print(\"ğŸ”´ Status: Drowsy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9db647",
   "metadata": {},
   "source": [
    "Build Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa23e6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Emotion sequences: 250\n",
      "âœ… Drowsy sequences: 608\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_data_from_dir(base_dir, img_size=(48, 48), grayscale=True):\n",
    "    data = []\n",
    "    for label in os.listdir(base_dir):\n",
    "        class_dir = os.path.join(base_dir, label)\n",
    "        if not os.path.isdir(class_dir):\n",
    "            continue\n",
    "        for fname in sorted(os.listdir(class_dir)):\n",
    "            fpath = os.path.join(class_dir, fname)\n",
    "            img = cv2.imread(fpath)\n",
    "            if img is None:\n",
    "                continue\n",
    "            if grayscale:\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            img = cv2.resize(img, img_size)\n",
    "            data.append((img, label))\n",
    "    return data\n",
    "\n",
    "# Load both datasets\n",
    "emotion_data = load_data_from_dir(EMOTION_PATH)\n",
    "drowsy_data = load_data_from_dir(DROWSY_PATH)\n",
    "\n",
    "def group_into_sequences(data, sequence_length=3, relabel_fn=lambda l: l):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for i in range(0, len(data) - sequence_length + 1, sequence_length):\n",
    "        seq = [data[j][0] for j in range(i, i + sequence_length)]\n",
    "        label = relabel_fn(data[i][1])\n",
    "        sequences.append(np.array(seq))\n",
    "        labels.append(label)\n",
    "    return sequences, labels\n",
    "\n",
    "# Define label mapping logic\n",
    "emotion_sequences, emotion_labels = group_into_sequences(\n",
    "    emotion_data, sequence_length=3, relabel_fn=lambda l: f\"Active:{l}\"\n",
    ")\n",
    "drowsy_sequences, drowsy_labels = group_into_sequences(\n",
    "    drowsy_data, sequence_length=15, relabel_fn=lambda l: \"Drowsy\" if l == \"Fatigue\" else \"Active\"\n",
    ")\n",
    "\n",
    "# Combine sequences and labels\n",
    "\n",
    "\n",
    "print(f\"âœ… Emotion sequences: {len(emotion_sequences)}\")\n",
    "print(f\"âœ… Drowsy sequences: {len(drowsy_sequences)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75519ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total combined sequences: 858\n"
     ]
    }
   ],
   "source": [
    "# Combine emotion and drowsy sequences and labels\n",
    "all_sequences = emotion_sequences + drowsy_sequences\n",
    "all_labels = emotion_labels + drowsy_labels\n",
    "\n",
    "print(f\"Total combined sequences: {len(all_sequences)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0f7d316c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example data structure from emotion_sequences:\n",
      "<class 'numpy.ndarray'>\n",
      "Length: 3\n",
      "First element type: <class 'numpy.ndarray'>\n",
      "Last element: [[ 27  13  10 ...  17 123 240]\n",
      " [ 19   9  14 ...   5  64 209]\n",
      " [ 14   7  17 ...   4  25 166]\n",
      " ...\n",
      " [222 224 226 ... 152 154 159]\n",
      " [229 230 232 ... 150 152 156]\n",
      " [233 233 233 ... 148 152 156]]\n",
      "Length: 3\n",
      "Length: 15\n"
     ]
    }
   ],
   "source": [
    "print(\"Example data structure from emotion_sequences:\")\n",
    "print(type(emotion_sequences[0]))\n",
    "print(f\"Length: {len(emotion_sequences[0])}\")\n",
    "print(f\"First element type: {type(emotion_sequences[0][0])}\")\n",
    "print(f\"Last element: {emotion_sequences[0][-1]}\")\n",
    "\n",
    "\n",
    "print(f\"Length: {len(emotion_sequences[0])}\")\n",
    "print(f\"Length: {len(drowsy_sequences[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c56a56ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (858,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_categorical\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Convert sequences and labels to NumPy arrays\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m X = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_sequences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m y = np.array(all_labels)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Encode string labels to integers\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (858,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Convert sequences and labels to NumPy arrays\n",
    "X = np.array(all_sequences)\n",
    "y = np.array(all_labels)\n",
    "\n",
    "# Encode string labels to integers\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "y_categorical = to_categorical(y_encoded)\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_categorical, test_size=0.2, random_state=42, stratify=y_categorical\n",
    ")\n",
    "\n",
    "# Show label mapping\n",
    "label_map = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(\"âœ… Label mapping:\", label_map)\n",
    "print(\"âœ… Training samples:\", X_train.shape)\n",
    "print(\"âœ… Test samples:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066b58fc",
   "metadata": {},
   "source": [
    "Merge and shuffle sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a053cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Total sequences: 858\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Merge both datasets\n",
    "all_sequences = emotion_sequences + drowsy_sequences\n",
    "print(f\"ğŸ“¦ Total sequences: {len(all_sequences)}\")\n",
    "\n",
    "# Shuffle the sequences\n",
    "all_sequences = shuffle(all_sequences, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe35e58",
   "metadata": {},
   "source": [
    "Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4289fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Train: 686 sequences\n",
      "ğŸ“Š Test:  172 sequences\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = [seq[0] for seq in all_sequences]\n",
    "y = [seq[1] for seq in all_sequences]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“Š Train: {len(X_train)} sequences\")\n",
    "print(f\"ğŸ“Š Test:  {len(X_test)} sequences\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69fd07f",
   "metadata": {},
   "source": [
    "Check before converting to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb423b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. First split raw lists\n",
    "X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 2. Function to filter and fix sequences\n",
    "def filter_valid_sequences(X_list, y_list):\n",
    "    filtered_X, filtered_y = [], []\n",
    "    for seq, label in zip(X_list, y_list):\n",
    "        if isinstance(seq, list) and len(seq) > 0:\n",
    "            frame_shapes = [f.shape for f in seq]\n",
    "            if all(s == frame_shapes[0] for s in frame_shapes):  # all same shape\n",
    "                filtered_X.append(np.array(seq))  # convert list of frames to 3D array\n",
    "                filtered_y.append(label)\n",
    "    return filtered_X, filtered_y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
